{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "import json\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "from PIL import Image\n",
    "import uuid\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from konlpy.tag import Hannanum\n",
    "from konlpy.tag import Kkma\n",
    "from konlpy.tag import Komoran\n",
    "import gensim\n",
    "from datetime import datetime\n",
    "import pprint\n",
    "from ultimate_emoji import deEmojify\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "api_url = 'https://6fg8wpedeo.apigw.ntruss.com/custom/v1/18871/e8c4cfbf6bd0343c951de9ffbcb29bc8accf6d042d71277e3f48d85488718ae3/general'\n",
    "secret_key = 'V211d2FOR2R5Y1RlanVNSkllZEh2emRDT3NWUlZKc3I='\n",
    "# 자기 이름을 넣어서 돌리도록 하자\n",
    "my_name = '정원'\n",
    "# 아래 쪽에 있는 search_key_word 바꿔서 검색어도 넣도록 하자\n",
    "# search_key_word = \"홍대 우동\"     #### <- 테스트 하고 싶은 검색어\n",
    "\n",
    "\n",
    "\n",
    "# 네이버 클라우드 가입해서 자기 id랑 secret 키를 넣도록 하자\n",
    "# 해당 사이트 참조 https://yunwoong.tistory.com/153\n",
    "# https://developers.naver.com/docs/serviceapi/search/blog/blog.md\n",
    "# 블로그 검색이랑 ocr 둘다 서비스 신청해아 함\n",
    "# 신청하고 아래 변수에 자기 키랑 시크릿으로 변경\n",
    "# client_id = \"wMXnGlU_z7J6RAmvtkw7\"  # <- 발급받은 client id \n",
    "# client_secret = \"QOrDBMkQ6W\"        # <- 발급받은 client secret \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Blog extraction 1\n",
    "\n",
    "def get_infer_text(file_path, secret_key, api_url):\n",
    "    files = [('file', open(file_path, 'rb'))]\n",
    "    \n",
    "    request_json = {'images': [{'format': 'png',\n",
    "                                    'name': 'demo'\n",
    "                                }],\n",
    "                        'requestId': str(uuid.uuid4()),\n",
    "                        'version': 'V2',\n",
    "                        'timestamp': int(round(time.time() * 1000))\n",
    "                    }\n",
    "    \n",
    "    payload = {'message': json.dumps(request_json).encode('UTF-8')}\n",
    "    \n",
    "    headers = {\n",
    "    'X-OCR-SECRET': secret_key,\n",
    "    }            \n",
    "    response = requests.request(\"POST\", api_url, headers=headers, data=payload, files=files)\n",
    "    result = response.json()\n",
    "    return result\n",
    "\n",
    "def find_fake_blog(last_text):\n",
    "    fake_keyword = ['제공','협찬']\n",
    "    genuine_keyword = ['NO협찬','NO광고','내돈내산']\n",
    "    \n",
    "    result = False\n",
    "    for text in last_text:\n",
    "        if text in genuine_keyword:\n",
    "            return False\n",
    "        if text in fake_keyword:\n",
    "            result = True\n",
    "    return result\n",
    "\n",
    "def no_space(text):\n",
    "    result_remove_both_side = re.sub(r\"^\\s+|\\s+$\", \"\", text)\n",
    "    return result_remove_both_side\n",
    "\n",
    "def print_last_text(texts, num_words):\n",
    "    result_text = []\n",
    "    count_text = 0\n",
    "    for i in range(-1, -len(texts), -1):\n",
    "        txt = no_space(texts[i])\n",
    "        if len(txt) != 1:\n",
    "            result_text.append(txt)\n",
    "            count_text += 1\n",
    "        if count_text == num_words:\n",
    "            break\n",
    "    return result_text\n",
    "\n",
    "def clean_doc(doc):\n",
    "    doc = re.sub(r'[^\\wㄱ-ㅎ가-힣]', ' ', doc)\n",
    "    return doc\n",
    "\n",
    "def contain_fake_keyword(last_ten_texts):    \n",
    "    corpus = [clean_doc(x) for x in last_ten_texts]\n",
    "    corpus = [i.strip() for i in corpus]\n",
    "    # han = Hannanum()\n",
    "    # komoran = Komoran() \n",
    "    kkma = Kkma()\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    ##이모티콘 삭제를 위해서\n",
    "    emoji_pattern = re.compile(\"[\" \n",
    "    u\"\\U0001F600-\\U0001F64F\" # emoticons \n",
    "    u\"\\U0001F300-\\U0001F5FF\" # symbols & pictographs \n",
    "    u\"\\U0001F680-\\U0001F6FF\" # transport & map symbols \n",
    "    u\"\\U0001F1E0-\\U0001F1FF\" # flags (iOS) \n",
    "    u\"\\U0001f926-\\U0001f937\" # maybe emoji\n",
    "         \"]+\", flags=re.UNICODE)\n",
    "    \"\"\"\n",
    "    \n",
    "    corpus = list(filter(None, corpus))\n",
    "    tokenized_corpus = [\n",
    "        kkma.morphs(deEmojify(doc)) for doc in corpus\n",
    "    ]\n",
    "    lexicon = gensim.corpora.Dictionary(tokenized_corpus)\n",
    "    fake_keyword = ['제공','협찬', '소정', '원고료', '수수료', '업체', '수수료', '시식', '이벤트', '쿠폰','제공받','서비스']\n",
    "    true_keyword = ['스탭', '스텝']\n",
    "    fake_count = 0\n",
    "    for x in lexicon.items():\n",
    "        if x[1] in fake_keyword:\n",
    "            fake_count += 1\n",
    "        if x[1] in true_keyword:\n",
    "            fake_count -= 1            \n",
    "    if fake_count > 1:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def get_img_path(index):\n",
    "    save_img_path = os.getcwd() + \"/untitled_img_\" + str(index) +\".png\"\n",
    "    if os.path.isfile(save_img_path):\n",
    "        os.remove(save_img_path)\n",
    "    return save_img_path\n",
    "\n",
    "def get_sticker_path(index):\n",
    "    save_sticker_path = os.getcwd() + \"/untitled_img_sticker_\" + str(index) +\".png\"\n",
    "    if os.path.isfile(save_sticker_path):\n",
    "        os.remove(save_sticker_path)\n",
    "    return save_sticker_path\n",
    "\n",
    "def remove_transparency(save_img_path):\n",
    "    img = Image.open(save_img_path)\n",
    "    img = img.convert(\"RGBA\")\n",
    "    datas = img.getdata()\n",
    "    \n",
    "    newData = []\n",
    "    cutOff = 15\n",
    "    \n",
    "    for item in datas:\n",
    "        if item[3] <= cutOff:\n",
    "            newData.append((255, 255, 255, 255))\n",
    "            # RGB의 각 요소가 모두 cutOff 이상이면 transparent하게 바꿔줍니다.\n",
    "        else:\n",
    "            newData.append(item)\n",
    "            # 나머지 요소는 변경하지 않습니다.\n",
    "    \n",
    "    img.putdata(newData)\n",
    "    img.save(save_img_path, \"PNG\") # PNG 포맷으로 저장합니다.        \n",
    "\n",
    "def contain_fake_image_keyword(img_list, img_sticker_list):\n",
    "    result_img = None\n",
    "    result_sticker = None\n",
    "    last_text = []\n",
    "    # save_img_path = \"/Users/seonmin.kim/Downloads/untitled_img.png\"#\"asset/untitled_img.png\"\n",
    "    # save_img_path = os.getcwd() + \"/untitled_img.png\"\n",
    "    # if os.path.isfile(save_img_path):\n",
    "    #     os.remove(save_img_path)\n",
    "        \n",
    "    # save_sticker_path = \"/Users/seonmin.kim/Downloads/untitled_img_sticker.png\"#\"asset/untitled_img_sticker.png\"\n",
    "    # save_sticker_path = os.getcwd() + \"/untitled_img_sticker.png\"\n",
    "    # if os.path.isfile(save_sticker_path):\n",
    "    #     os.remove(save_sticker_path)            \n",
    "        \n",
    "    if img_list:\n",
    "        for img_index in range(0,len(img_list)):\n",
    "            path = img_list.pop()\n",
    "            save_img_path = get_img_path(img_index)\n",
    "            urllib.request.urlretrieve(path, save_img_path)\n",
    "            remove_transparency(save_img_path)\n",
    "            result_img = get_infer_text(save_img_path, secret_key, api_url)\n",
    "\n",
    "            if result_img['images'][0]['inferResult'] != 'ERROR':\n",
    "                for field in result_img['images'][0]['fields']:\n",
    "                    last_text.append(field['inferText'])\n",
    "                    # print(field['inferText'])\n",
    "            if img_index == 3: # 마지막 3개이 image만 ocr 판독하여 검증\n",
    "                break\n",
    "        \n",
    "    if img_sticker_list:\n",
    "        for img_index in range(0,len(img_sticker_list)):\n",
    "            path = img_sticker_list.pop()     \n",
    "            save_sticker_path = get_sticker_path(img_index)              \n",
    "            urllib.request.urlretrieve(path, save_sticker_path)\n",
    "            result_sticker = get_infer_text(save_sticker_path, secret_key, api_url)\n",
    "            \n",
    "            if result_sticker['images'][0]['inferResult'] != 'ERROR':\n",
    "                for field in result_sticker['images'][0]['fields']:\n",
    "                    last_text.append(field['inferText'])\n",
    "                    # print(field['inferText'])\n",
    "            if img_index == 3: # 마지막 3개이 sticker image만 ocr 판독하여 검증\n",
    "                break\n",
    "    \n",
    "    fake_result = contain_fake_keyword(last_text)\n",
    "    return fake_result\n",
    "\n",
    "\n",
    "def get_num_nouns(full_texts):    \n",
    "    corpus = [clean_doc(x) for x in full_texts]\n",
    "    # han = Hannanum()\n",
    "    # komoran = Komoran() \n",
    "    kkma = Kkma()\n",
    "    tokenized_corpus = [\n",
    "        kkma.nouns(doc) for doc in corpus\n",
    "    ]\n",
    "    lexicon = gensim.corpora.Dictionary(tokenized_corpus)\n",
    "    \n",
    "    return len(lexicon.items())\n",
    "\n",
    "def get_num_sentetnces(full_texts):    \n",
    "    corpus = [clean_doc(x) for x in full_texts]\n",
    "    # han = Hannanum()\n",
    "    # komoran = Komoran() \n",
    "    kkma = Kkma()\n",
    "    tokenized_corpus = [\n",
    "        kkma.sentences(doc) for doc in corpus\n",
    "    ]\n",
    "    lexicon = gensim.corpora.Dictionary(tokenized_corpus)\n",
    "    \n",
    "    return len(lexicon.items())\n",
    "\n",
    "def get_num_pos(full_texts):   \n",
    "     \n",
    "    corpus = [clean_doc(x) for x in full_texts]\n",
    "    # han = Hannanum()\n",
    "    # komoran = Komoran() \n",
    "    kkma = Kkma()\n",
    "    tokenized_corpus = [\n",
    "        kkma.pos(doc) for doc in corpus\n",
    "    ]\n",
    "    lexicon = gensim.corpora.Dictionary(tokenized_corpus)\n",
    "    print(lexicon)\n",
    "    return lexicon.items()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Image extraction\n",
    "\n",
    "\n",
    "# 블로그 방문 함수\n",
    "from numpy import savez_compressed\n",
    "\n",
    "\n",
    "def visit_blog_by_chrome(url):\n",
    "    headers = { 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36',\n",
    "               'lang':'ko'}\n",
    "    # driver = webdriver.Chrome(executable_path=r'/Users/seonmin.kim/Downloads/chromedriver') # 웹드라이버가 설치된 경로를 지정해주시면 됩니다.\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "    time.sleep(0.5)\n",
    "    html = driver.page_source  # 링크를 따와야 하므로 게시글이 많이 불러진 상태에서 html 객체를 생성\n",
    "    return html\n",
    "\n",
    "\n",
    "#블로그 이미지 추출 함수\n",
    "def get_img_list(html):\n",
    "    try:\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        soup = BeautifulSoup(soup.decode(False, formatter=None), None)\n",
    "        imgs = soup.select('.se-image-resource')\n",
    "        imgs_sticker = soup.select('.se-sticker-image')\n",
    "        img_list = list()\n",
    "        img_sticker_list = list()\n",
    "        if soup.find(\"div\", attrs={\"class\":\"se-main-container\"}): # 스마트 에디터로 작성한 경우\n",
    "            imgs = soup.find(\"div\", attrs={\"class\":\"se-main-container\"}).select('.se-image-resource')  # 큰 사진\n",
    "            imgs_sticker = soup.find(\"div\", attrs={\"class\":\"se-main-container\"}).select('.se-sticker-image') # 스티커 이미지\n",
    "            for img in imgs:\n",
    "                if img.get('data-lazy-src') is not None:\n",
    "                    img_list.append(img['data-lazy-src'])  # 큰사진의 경로\n",
    "                else:\n",
    "                    img_list.append(img['src'])  # 작은 사진의 경로 경로\n",
    "            for img in imgs_sticker:\n",
    "                if img.get('src') is not None:\n",
    "                    img_sticker_list.append(img['src'])  # 스티커 사진의 경로\n",
    "                else:\n",
    "                    print(f\"error on sticker image : {img}\")\n",
    "        else:                            \n",
    "            imgs = soup.select('.se-image-resource')\n",
    "            imgs_sticker = soup.select('.se-sticker-image')\n",
    "            for img in imgs:\n",
    "                if img.get('data-lazy-src') is not None:\n",
    "                    img_list.append(img['data-lazy-src'])  # 큰사진의 경로\n",
    "                else:\n",
    "                    img_list.append(img['src'])  # 작은 사진의 경로\n",
    "            for img in imgs_sticker:\n",
    "                if img.get('src') is not None:\n",
    "                    img_sticker_list.append(img['src'])  # 스티커 사진의 경로\n",
    "                else:\n",
    "                    print(f\"error on sticker image : {img}\")\n",
    "            \n",
    "        return img_list, img_sticker_list\n",
    "    except:\n",
    "        print(f\"err {img}\")\n",
    "\n",
    "\n",
    " # 저장경로 생성 함수\n",
    "def mkdir(title):\n",
    "    # save_dest = '/Users/seonmin.kim/Downloads/'# for window - 'C:/Users/root/Desktop/'  # 저장을 위한 베이스 경로\n",
    "    save_dest = os.getcwd()\n",
    "    if os.path.isdir(save_dest + title):  # 베이스경로에 제목으로 하위 폴더 생성\n",
    "        pass\n",
    "    else:\n",
    "        os.makedirs(save_dest + title)\n",
    "    return save_dest + title  \n",
    "\n",
    "\n",
    "# 이미지 저장 함수\n",
    "def save_img(img_list, save_dest): \n",
    "    for i in range(0, len(img_list), 1): \n",
    "        ext_idx = (img_list[i].find('?type')) #확장자 찾기 위해 type 문자열을 먼저 찾고 \n",
    "        ext = (img_list[i][ext_idx-3:ext_idx]) #위에서 찾은 인덱스에서 -3하여 해당 파일의 확장자 확인 \n",
    "        urllib.request.urlretrieve(img_list[i], save_dest+'/'+str(i+1)+'.'+ext) \n",
    "        return len(img_list) \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### feature 함수들\n",
    "\n",
    "# 블로그 제목 추출 함수\n",
    "def get_title(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    contents = str(soup.select('.se-fs-'))  # 제목 추출을 위한 셀렉터\n",
    "    hangul = re.compile('[^ ㄱ-ㅣ가-힣]+')  # 한글만 따오기 위해\n",
    "    result = hangul.sub('', contents)\n",
    "    title = result.strip().split('  ')  # 공백으로 구분\n",
    "    return title[0]  # 0번째 인덱스가 제목 \n",
    "\n",
    "def get_blog_totalnumber(html): #블로그 전체 게시글 수\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    soup = BeautifulSoup(soup.decode(False, formatter=None), None)\n",
    "    contents=[]\n",
    "    texts = soup.find_all('h4', {'class': 'category_title pcol2'})\n",
    "    for p_span in texts:\n",
    "        # pdb.set_trace()\n",
    "        number = p_span.getText().strip().split(' ')[1][:-2]\n",
    "        contents.append(number)\n",
    "    return contents\n",
    "\n",
    "def visit_blog(url):\n",
    "    headers = { 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36',\n",
    "                'lang':'ko'}\n",
    "    html = requests.get(url, headers=headers).text\n",
    "    return html\n",
    "\n",
    "def input_url(input_url):\n",
    "    base_url = 'https://blog.naver.com/PostView.naver?blogId='  # 크롤링을 위한 베이스 주소\n",
    "    # input_url = input('블로그 주소를 입력하세요. 형식은 아래와 같습니다\\nhttps://blog.naver.com/아이디/게시글번호\\n')\n",
    "    id = input_url.split('/')[3]  # 아이디 추출\n",
    "    number = input_url.split('/')[4]  # 게시글번호 추출\n",
    "    url = base_url + id + '&logNo=' + number\n",
    "    return url\n",
    "\n",
    "\n",
    "def get_blog_number(html): #해당 블로그가 속한 메뉴의 게시물의 수\n",
    "    \n",
    "    soup1 = BeautifulSoup(html,'html.parser')\n",
    "    soup1 = BeautifulSoup(soup1.decode(False, formatter=None), None)\n",
    "    food_info = soup1.find(\"div\", {\"class\": \"blog2_series\"})\n",
    "    if food_info:\n",
    "        link = food_info.find(\"a\", {\"class\": \"pcol2\"})\n",
    "        source_url = link['href'] if link else '' \n",
    "\n",
    "    tempUrl = \"https://blog.naver.com\" + source_url\n",
    "    htm12 = visit_blog(tempUrl)\n",
    "\n",
    "    soup2 = BeautifulSoup(htm12,'html.parser')\n",
    "    soup2 = BeautifulSoup(soup2.decode(False, formatter=None), None)\n",
    "\n",
    "    contents=[]\n",
    "    texts = soup2.find_all('h4', {'class': 'category_title pcol2'})\n",
    "    for p_span in texts:\n",
    "        # pdb.set_trace()\n",
    "        temp = p_span.getText().strip().split()    \n",
    "        number = [ x[:-2] for x in temp if x[-2:] == \"개의\"]        \n",
    "        contents.append(number)\n",
    "    return contents[0]\n",
    "\n",
    "def feature_sisAutoPlayYN(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    soup = BeautifulSoup(soup.decode(False, formatter=None), None)\n",
    "    t = soup.find_all('script')\n",
    "    #print(t)\n",
    "    a = re.findall(r'\\sisAutoPlayYN\\s=\\s\\w{5}', str(t))\n",
    "    if 'true' in a:\n",
    "        a = True\n",
    "    else:\n",
    "        a = False\n",
    "    return a\n",
    "\n",
    "#블로거 프로필 info\n",
    "def get_profile_info(profile_html): #블로거 프로필 info\n",
    "    soup = BeautifulSoup(profile_html,'html.parser')\n",
    "    soup = BeautifulSoup(soup.decode(False, formatter=None), None)\n",
    "    name = []\n",
    "    texts = soup.find_all('table', {'class': 'profile_info'})\n",
    "    for text in texts:\n",
    "        name_ = text.getText().strip().split('\\n')[1]\n",
    "        name.append(name_)\n",
    "    \n",
    "    return name\n",
    "\n",
    "def get_profile_info1(profile_html): #블로거 프로필 info\n",
    "    soup = BeautifulSoup(profile_html,'html.parser')\n",
    "    soup = BeautifulSoup(soup.decode(False, formatter=None), None)\n",
    "    gender = []\n",
    "    texts = soup.find_all('table', {'class': 'profile_info'})\n",
    "    for text in texts:\n",
    "        t_gender = text.getText().strip().split('\\n')[5]\n",
    "        gender.append(t_gender)\n",
    "    \n",
    "    return gender\n",
    "\n",
    "\n",
    "\n",
    "def clean_doc(doc):\n",
    "    doc = re.sub(r'[^\\w]ㄱ-ㅎ가-힣',' ',doc)\n",
    "    return doc\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "def body_length(texts): #본문길이(공백포함)\n",
    "    temp = None\n",
    "    body_length = []\n",
    "    texts = [clean_doc(x) for x in texts]\n",
    "    texts = ''.join(texts)\n",
    "    temp = len(texts)\n",
    "    body_length.append(temp)\n",
    "    \n",
    "    return body_length\n",
    "\"\"\"\n",
    "def title_length(texts): #제목길이(단어 공백포함)\n",
    "    try:\n",
    "        temp = None\n",
    "        title_length = []\n",
    "        texts = [clean_doc(x) for x in texts]\n",
    "        texts = ''.join(texts)\n",
    "        temp = len(texts)\n",
    "        title_length.append(temp)\n",
    "    \n",
    "        return title_length\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def comment_count(html): #블로그 댓글\n",
    "    characters = \"'?!\"\n",
    "    comment_c = []\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    soup = BeautifulSoup(soup.decode(False, formatter=None), None)\n",
    "    comments = soup.find_all(\"em\", {\"id\": \"commentCount\"})\n",
    "    if comments:\n",
    "        for c in comments:\n",
    "            temp = c.getText().strip().split()\n",
    "            temp = ''.join( x for x in temp if x not in characters)\n",
    "            comment_c.append(temp)\n",
    "\n",
    "        return comment_c\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "# 블로그 제목 추출 함수\n",
    "def get_last_text(html):\n",
    "    try:\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        soup = BeautifulSoup(soup.decode(False, formatter=None), None)\n",
    "        contents = []\n",
    "        if soup.find(\"div\", attrs={\"class\":\"se-main-container\"}): # 스마트 에디터로 작성한 경우\n",
    "            texts = soup.find(\"div\", attrs={\"class\":\"se-main-container\"}).find_all('div', {'class': re.compile('^se-module se-module-tex.*')})\n",
    "            for p_span in texts:\n",
    "                for txt in p_span.find_all('span'):\n",
    "                    contents.append(txt.get_text())\n",
    "        else:\n",
    "            texts = soup.find_all('div', {'class': re.compile('^se-module se-module-tex.*')})\n",
    "            for p_span in texts:\n",
    "                for txt in p_span.find_all('span'):\n",
    "                    contents.append(txt.get_text())\n",
    "        return contents\n",
    "    except:\n",
    "        print(f\"err text\")    \n",
    "\n",
    "def profile_url(input_url):\n",
    "    base_url = 'https://blog.naver.com/profile/intro.naver?blogId='  # 크롤링을 위한 베이스 주소\n",
    "    # input_url = input('블로그 주소를 입력하세요. 형식은 아래와 같습니다\\nhttps://blog.naver.com/아이디/게시글번호\\n')\n",
    "    id = input_url.split('/')[3]  # 아이디 추출\n",
    "    profile_url = base_url + id\n",
    "    profile_html = visit_blog(profile_url)\n",
    "    return profile_html   \n",
    "\n",
    "def text_length(texts): #본문길이(공백포함)\n",
    "    try:\n",
    "        temp = None\n",
    "        body_length = []\n",
    "        texts = [clean_doc(x) for x in texts]\n",
    "        texts = ''.join(texts)\n",
    "        temp = len(texts)\n",
    "        body_length.append(temp)\n",
    "    \n",
    "        return body_length\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "\n",
    "# 블로그 해시태그 개수 및 내용 추출 함수\n",
    "def get_num_hashtags(html):\n",
    "    try:\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        soup = BeautifulSoup(soup.decode(False, formatter=None), None)\n",
    "        \n",
    "        post_footer = soup.find(\"div\", attrs={\"class\":\"post_footer_contents\"})\n",
    "        post_footer = post_footer.select(\"span\", attrs={\"class\":\"ell\"})\n",
    "        blog_hashtags = []\n",
    "        for item in post_footer:\n",
    "            parsed_item = item.get_text().strip()\n",
    "            if parsed_item != '' and parsed_item != '취소\\n확인':\n",
    "                blog_hashtags.append(parsed_item)\n",
    "        return blog_hashtags\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "# 블로그 공감 개수 추출 함수\n",
    "def get_num_likes(html):\n",
    "    try:\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        soup = BeautifulSoup(soup.decode(False, formatter=None), None)\n",
    "        \n",
    "        likeit = soup.find(\"span\", attrs={\"class\":\"u_likeit_list_btn\"})\n",
    "        likeit_count = likeit.find(\"em\", attrs={\"class\":\"u_cnt\"}).get_text()\n",
    "        return likeit_count\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "# 블로그 이웃 개수 추출 함수\n",
    "def get_num_neibors(html):\n",
    "    try:\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        soup = BeautifulSoup(soup.decode(False, formatter=None), None)\n",
    "        \n",
    "        num_neibors = soup.find(\"div\", attrs={\"id\":\"blog-stat\"})\n",
    "        num_neibors = num_neibors.find(\"ul\", attrs={\"class\":\"info\"})\n",
    "        num_neibors = num_neibors.select('em')\n",
    "        blog_stats = []\n",
    "        for item in num_neibors:\n",
    "            blog_stats.append(item.get_text().strip())\n",
    "        return blog_stats   # 블로그 이웃, 글 보내기, 글 스크랩 순으로 list에 추가\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "    \n",
    "### weekday, time_delta, datetime(post)        \n",
    "from datetime import timedelta\n",
    "default_date = datetime(2022, 1, 1, 0, 0, 0)\n",
    "def post_datetime(html):\n",
    "    try:\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        soup = BeautifulSoup(soup.decode(False, formatter=None), None)\n",
    "        \n",
    "        if soup.find(\"span\", attrs={\"se_publishDate pcol2\"}):\n",
    "            datetime_str = soup.find(\"span\", attrs={\"se_publishDate pcol2\"}).get_text().replace(\" \" , \"\")\n",
    "            if re.search(r'시간', datetime_str):\n",
    "                now = datetime.now()\n",
    "                hour = re.sub(r'[^0-9]', '', datetime_str)\n",
    "                datetime_str = now - timedelta(hours = int(hour))\n",
    "                datetime_str = datetime_str.strftime('%Y.%m.%d.%H:%M')\n",
    "            elif re.search(r'분', datetime_str):\n",
    "                now = datetime.now()\n",
    "                minute = re.sub(r'[^0-9]', '', datetime_str)\n",
    "                datetime_str = now - timedelta(minutes = int(minute))\n",
    "                datetime_str = datetime_str.strftime('%Y.%m.%d.%H:%M')\n",
    "                \n",
    "        elif soup.find(\"p\", attrs={\"date fil5 pcol2 _postAddDate\"}):\n",
    "            datetime_str = soup.find(\"p\", attrs={\"date fil5 pcol2 _postAddDate\"}).get_text().replace(\" \" , \"\")\n",
    "        \n",
    "        datetime_obj = datetime.strptime(datetime_str, '%Y.%m.%d.%H:%M')\n",
    "        time_delta = (datetime_obj - default_date).days\n",
    "        post_weekday = datetime_obj.weekday()\n",
    "        return time_delta, post_weekday, str(datetime_obj) #, datetime_obj#, post_weekday\n",
    "    except:\n",
    "        return 0\n",
    "        \n",
    "### 예약하기 외부 링크 버튼 제공 여/부\n",
    "def get_reserved(html):\n",
    "    try:\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        soup = BeautifulSoup(soup.decode(False, formatter=None), None)\n",
    "        contents = []\n",
    "        texts = soup.find_all(\"a\", attrs={\"class\":\"se-placesMap-additional-button se-placesMap-button-reservation\"})\n",
    "        if texts:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except:\n",
    "        return 0\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fake_recognition(fake_result_text, fake_result_text_):\n",
    "    if fake_result_text_ or fake_result_text:\n",
    "        return 1 #print(f\"해당 블로그는 가짜 리뷰를 작성하였습니다.\")\n",
    "    else:\n",
    "        return 0 #print(f\"해당 블로그는 진짜 리뷰로 판단됩니다.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\won\\AppData\\Local\\Temp\\ipykernel_29956\\2263356920.py:111: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  output = output.append(new_df, ignore_index = True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "client_id = \"wMXnGlU_z7J6RAmvtkw7\"  # <- 발급받은 client id \n",
    "client_secret = \"QOrDBMkQ6W\"        # <- 발급받은 client secret \n",
    "search_key_word = \"홍대 우동\"     #### <- 테스트 하고 싶은 검색어\n",
    "encText = urllib.parse.quote(search_key_word)   # <- search keyword \n",
    "num_display = 100\n",
    "num_start = 1\n",
    "url = \"https://openapi.naver.com/v1/search/blog?query=\" + encText \\\n",
    "        + \"&display=\" + str(num_display) + \"&start=\" + str(num_start) # json 결과\n",
    "# url = \"https://openapi.naver.com/v1/search/blog.xml?query=\" + encText # xml 결과\n",
    "request = urllib.request.Request(url)\n",
    "request.add_header(\"X-Naver-Client-Id\",client_id)\n",
    "request.add_header(\"X-Naver-Client-Secret\",client_secret)\n",
    "response = urllib.request.urlopen(request)\n",
    "rescode = response.getcode()\n",
    "\n",
    "\n",
    "if(rescode==200):\n",
    "    response_body = json.load(response)\n",
    "else:\n",
    "    print(\"Error Code:\" + rescode)\n",
    "\n",
    "\n",
    "url1 = [] # 0\n",
    "java_feature = [] # 1\n",
    "contents = [] # 2\n",
    "profile_html = [] # 3\n",
    "name = [] # 4\n",
    "gender = [] # 5\n",
    "size_text = [] # 6\n",
    "size_title = [] # 7\n",
    "com_count = [] # 8\n",
    "num_neibors = [] #9\n",
    "num_likes = [] #10\n",
    "num_hashtags = [] #11\n",
    "img_list_size = []\n",
    "img_sticker_size = []\n",
    "fake_result = []\n",
    "post_timedelta = []\n",
    "post_week = []\n",
    "post_date = []\n",
    "get_reserved_link = []\n",
    "output = pd.DataFrame()\n",
    "\n",
    "### [:10] 이 부분에서 불러오는 개수 설정 가능\n",
    "for item in response_body['items'][10:20]:\n",
    "\n",
    "    # url = input_url(item['link'])\n",
    "    # html = visit_blog(url)\n",
    "    # title = get_title(html)\n",
    "    # texts = get_last_text(html)\n",
    "    # java_feature.append(feature_sisAutoPlayYN(html))\n",
    "    # contents.append(get_blog_totalnumber(html))\n",
    "    # profile_html = profile_url(item['link'])\n",
    "    # name.append(get_profile_info(profile_html))\n",
    "    # gender.append(get_profile_info1(profile_html))\n",
    "    # size_text.append(text_length(texts))\n",
    "    # size_title.append(title_length(title))\n",
    "    # com_count.append(comment_count(html))\n",
    "\n",
    "\n",
    "    url = input_url(item['link'])\n",
    "    html = visit_blog(url)\n",
    "    title = get_title(html)\n",
    "    texts = get_last_text(html)\n",
    "    java_feature = (feature_sisAutoPlayYN(html))\n",
    "    contents = (get_blog_totalnumber(html))\n",
    "    profile_html = profile_url(item['link'])\n",
    "    name = (get_profile_info(profile_html))\n",
    "    gender = (get_profile_info1(profile_html))\n",
    "    size_text = (text_length(texts))\n",
    "    size_title = (title_length(title))\n",
    "    com_count = (comment_count(html))\n",
    "    num_neibors = (get_num_neibors(html))\n",
    "    num_likes = (get_num_likes(html))\n",
    "    num_hashtags = len(get_num_hashtags(html))\n",
    "    img_list, img_sticker_list = get_img_list(html)\n",
    "    img_list_size = len(img_list)\n",
    "    img_sticker_size = len(img_sticker_list)\n",
    "    last_ten_texts = print_last_text(texts, 10)\n",
    "    post_timedelta, post_week, post_date = post_datetime(html)\n",
    "    get_reserved_link = get_reserved(html)\n",
    "    fake_result_text_ = contain_fake_keyword(last_ten_texts)\n",
    "    fake_result_text = contain_fake_image_keyword(img_list, img_sticker_list)\n",
    "    fake_result = fake_recognition(fake_result_text, fake_result_text_)\n",
    "\n",
    "    \n",
    "\n",
    "    new_df = pd.DataFrame({\n",
    "    \"url\" : url,\n",
    "    \"java_feature\" : java_feature,\n",
    "    \"contents\" : contents,\n",
    "   # \"profile_html\" : profile_html,\n",
    "    \"name\" : name,\n",
    "    \"gender\" : gender,\n",
    "    \"size_text\" : size_text,\n",
    "    \"size_title\" : size_title,\n",
    "    \"com_count\" : com_count,\n",
    "    \"num_neibors\" : num_neibors,\n",
    "    \"num_likes\" : num_likes,\n",
    "    \"num_hashtags\" : num_hashtags,\n",
    "    \"img_list_size\" : img_list_size,\n",
    "    \"img_sticker_size\" : img_sticker_size,\n",
    "    \"post_timedelta\" : post_timedelta,\n",
    "    \"post_week\" : post_week,\n",
    "    \"post_date\" : post_date,\n",
    "    \"get_reserved_link\": get_reserved_link,\n",
    "    \"fake_result\" : fake_result\n",
    "        \n",
    "    })\n",
    "\n",
    "    output = output.append(new_df, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 18)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "output['id'] = output['url']\n",
    "output['input_user'] = my_name\n",
    "output.to_json('saved_results.json',orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2d89c2dbec9312f7dcc350e1cb21f4f3643aeaec3ef821a13356311b9b897efe"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "52013e79db560264923fac81a2b1f3585321d9359d38a153ae67e5b5717e752b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
