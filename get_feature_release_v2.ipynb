{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "import json\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "from PIL import Image\n",
    "import uuid\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from konlpy.tag import Hannanum\n",
    "from konlpy.tag import Kkma\n",
    "from konlpy.tag import Komoran\n",
    "import gensim\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "api_url = 'https://6fg8wpedeo.apigw.ntruss.com/custom/v1/18871/e8c4cfbf6bd0343c951de9ffbcb29bc8accf6d042d71277e3f48d85488718ae3/general'\n",
    "secret_key = 'V211d2FOR2R5Y1RlanVNSkllZEh2emRDT3NWUlZKc3I='"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Blog extraction 1\n",
    "\n",
    "def get_infer_text(file_path, secret_key, api_url):\n",
    "    files = [('file', open(file_path, 'rb'))]\n",
    "    \n",
    "    request_json = {'images': [{'format': 'png',\n",
    "                                    'name': 'demo'\n",
    "                                }],\n",
    "                        'requestId': str(uuid.uuid4()),\n",
    "                        'version': 'V2',\n",
    "                        'timestamp': int(round(time.time() * 1000))\n",
    "                    }\n",
    "    \n",
    "    payload = {'message': json.dumps(request_json).encode('UTF-8')}\n",
    "    \n",
    "    headers = {\n",
    "    'X-OCR-SECRET': secret_key,\n",
    "    }            \n",
    "    response = requests.request(\"POST\", api_url, headers=headers, data=payload, files=files)\n",
    "    result = response.json()\n",
    "    return result\n",
    "\n",
    "def find_fake_blog(last_text):\n",
    "    fake_keyword = ['제공','협찬']\n",
    "    genuine_keyword = ['NO협찬','NO광고','내돈내산']\n",
    "    \n",
    "    result = False\n",
    "    for text in last_text:\n",
    "        if text in genuine_keyword:\n",
    "            return False\n",
    "        if text in fake_keyword:\n",
    "            result = True\n",
    "    return result\n",
    "\n",
    "def no_space(text):\n",
    "    result_remove_both_side = re.sub(r\"^\\s+|\\s+$\", \"\", text)\n",
    "    return result_remove_both_side\n",
    "\n",
    "def print_last_text(texts, num_words):\n",
    "    result_text = []\n",
    "    count_text = 0\n",
    "    for i in range(-1, -len(texts), -1):\n",
    "        txt = no_space(texts[i])\n",
    "        if len(txt) != 1:\n",
    "            result_text.append(txt)\n",
    "            count_text += 1\n",
    "        if count_text == num_words:\n",
    "            break\n",
    "    return result_text\n",
    "\n",
    "def clean_doc(doc):\n",
    "    doc = re.sub(r'[^\\wㄱ-ㅎ가-힣]', ' ', doc)\n",
    "    return doc\n",
    "\n",
    "def contain_fake_keyword(last_ten_texts):    \n",
    "    corpus = [clean_doc(x) for x in last_ten_texts]\n",
    "    # han = Hannanum()\n",
    "    # komoran = Komoran() \n",
    "    kkma = Kkma()\n",
    "\n",
    "    ##이모티콘 삭제를 위해서\n",
    "    emoji_pattern = re.compile(\"[\" \n",
    "    u\"\\U0001F600-\\U0001F64F\" # emoticons \n",
    "    u\"\\U0001F300-\\U0001F5FF\" # symbols & pictographs \n",
    "    u\"\\U0001F680-\\U0001F6FF\" # transport & map symbols \n",
    "    u\"\\U0001F1E0-\\U0001F1FF\" # flags (iOS) \n",
    "         \"]+\", flags=re.UNICODE)    \n",
    "\n",
    "    tokenized_corpus = [\n",
    "        kkma.morphs(emoji_pattern.sub(r'', doc)) for doc in corpus\n",
    "    ]\n",
    "    lexicon = gensim.corpora.Dictionary(tokenized_corpus)\n",
    "    fake_keyword = ['제공','협찬', '소정', '원고료', '수수료', '업체', '수수료', '시식', '이벤트', '쿠폰']\n",
    "    fake_count = 0\n",
    "    for x in lexicon.items():\n",
    "        if x[1] in fake_keyword:\n",
    "            fake_count += 1\n",
    "    if fake_count > 1:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def contain_fake_image_keyword(img_list, img_sticker_list):\n",
    "    result_img = None\n",
    "    result_sticker = None\n",
    "    last_text = []\n",
    "    # save_img_path = \"/Users/seonmin.kim/Downloads/untitled_img.png\"#\"asset/untitled_img.png\"\n",
    "    save_img_path = os.getcwd() + \"/untitled_img.png\"\n",
    "    if os.path.isfile(save_img_path):\n",
    "        os.remove(save_img_path)\n",
    "        \n",
    "    # save_sticker_path = \"/Users/seonmin.kim/Downloads/untitled_img_sticker.png\"#\"asset/untitled_img_sticker.png\"\n",
    "    save_sticker_path = os.getcwd() + \"/untitled_img_sticker.png\"\n",
    "    if os.path.isfile(save_sticker_path):\n",
    "        os.remove(save_sticker_path)            \n",
    "        \n",
    "    if img_list:\n",
    "        path = img_list.pop()\n",
    "        urllib.request.urlretrieve(path, save_img_path)\n",
    "        result_img = get_infer_text(save_img_path, secret_key, api_url)\n",
    "\n",
    "        if result_img['images'][0]['inferResult'] != 'ERROR':\n",
    "            for field in result_img['images'][0]['fields']:\n",
    "                last_text.append(field['inferText'])    \n",
    "        \n",
    "    if img_sticker_list:\n",
    "        path = img_sticker_list.pop()                  \n",
    "        urllib.request.urlretrieve(path, save_sticker_path)\n",
    "        result_sticker = get_infer_text(save_sticker_path, secret_key, api_url)\n",
    "        \n",
    "        if result_sticker['images'][0]['inferResult'] != 'ERROR':\n",
    "            for field in result_sticker['images'][0]['fields']:\n",
    "                last_text.append(field['inferText'])\n",
    "    #print(last_text)\n",
    "    \n",
    "    \n",
    "    fake_result = contain_fake_keyword(last_text)\n",
    "    return fake_result\n",
    "\n",
    "\n",
    "def get_num_nouns(full_texts):    \n",
    "    corpus = [clean_doc(x) for x in full_texts]\n",
    "    # han = Hannanum()\n",
    "    # komoran = Komoran() \n",
    "    kkma = Kkma()\n",
    "    tokenized_corpus = [\n",
    "        kkma.nouns(doc) for doc in corpus\n",
    "    ]\n",
    "    lexicon = gensim.corpora.Dictionary(tokenized_corpus)\n",
    "    \n",
    "    return len(lexicon.items())\n",
    "\n",
    "def get_num_sentetnces(full_texts):    \n",
    "    corpus = [clean_doc(x) for x in full_texts]\n",
    "    # han = Hannanum()\n",
    "    # komoran = Komoran() \n",
    "    kkma = Kkma()\n",
    "    tokenized_corpus = [\n",
    "        kkma.sentences(doc) for doc in corpus\n",
    "    ]\n",
    "    lexicon = gensim.corpora.Dictionary(tokenized_corpus)\n",
    "    \n",
    "    return len(lexicon.items())\n",
    "\n",
    "def get_num_pos(full_texts):   \n",
    "     \n",
    "    corpus = [clean_doc(x) for x in full_texts]\n",
    "    # han = Hannanum()\n",
    "    # komoran = Komoran() \n",
    "    kkma = Kkma()\n",
    "    tokenized_corpus = [\n",
    "        kkma.pos(doc) for doc in corpus\n",
    "    ]\n",
    "    lexicon = gensim.corpora.Dictionary(tokenized_corpus)\n",
    "    print(lexicon)\n",
    "    return lexicon.items()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Image extraction\n",
    "\n",
    "\n",
    "# 블로그 방문 함수\n",
    "from numpy import savez_compressed\n",
    "\n",
    "\n",
    "def visit_blog_by_chrome(url):\n",
    "    headers = { 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36',\n",
    "               'lang':'ko'}\n",
    "    # driver = webdriver.Chrome(executable_path=r'/Users/seonmin.kim/Downloads/chromedriver') # 웹드라이버가 설치된 경로를 지정해주시면 됩니다.\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "    time.sleep(0.5)\n",
    "    html = driver.page_source  # 링크를 따와야 하므로 게시글이 많이 불러진 상태에서 html 객체를 생성\n",
    "    return html\n",
    "\n",
    "\n",
    "#블로그 이미지 추출 함수\n",
    "def get_img_list(html):\n",
    "    try:\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        soup = BeautifulSoup(soup.decode(False, formatter=None), None)\n",
    "        imgs = soup.select('.se-image-resource')\n",
    "        imgs_sticker = soup.select('.se-sticker-image')\n",
    "        img_list = list()\n",
    "        img_sticker_list = list()\n",
    "        if soup.find(\"div\", attrs={\"class\":\"se-main-container\"}): # 스마트 에디터로 작성한 경우\n",
    "            imgs = soup.find(\"div\", attrs={\"class\":\"se-main-container\"}).select('.se-image-resource')  # 큰 사진\n",
    "            imgs_sticker = soup.find(\"div\", attrs={\"class\":\"se-main-container\"}).select('.se-sticker-image') # 스티커 이미지\n",
    "            for img in imgs:\n",
    "                if img.get('data-lazy-src') is not None:\n",
    "                    img_list.append(img['data-lazy-src'])  # 큰사진의 경로\n",
    "                else:\n",
    "                    img_list.append(img['src'])  # 작은 사진의 경로 경로\n",
    "            for img in imgs_sticker:\n",
    "                if img.get('src') is not None:\n",
    "                    img_sticker_list.append(img['src'])  # 스티커 사진의 경로\n",
    "                else:\n",
    "                    print(f\"error on sticker image : {img}\")\n",
    "        else:                            \n",
    "            imgs = soup.select('.se-image-resource')\n",
    "            imgs_sticker = soup.select('.se-sticker-image')\n",
    "            for img in imgs:\n",
    "                if img.get('data-lazy-src') is not None:\n",
    "                    img_list.append(img['data-lazy-src'])  # 큰사진의 경로\n",
    "                else:\n",
    "                    img_list.append(img['src'])  # 작은 사진의 경로\n",
    "            for img in imgs_sticker:\n",
    "                if img.get('src') is not None:\n",
    "                    img_sticker_list.append(img['src'])  # 스티커 사진의 경로\n",
    "                else:\n",
    "                    print(f\"error on sticker image : {img}\")\n",
    "            \n",
    "        return img_list, img_sticker_list\n",
    "    except:\n",
    "        print(f\"err {img}\")\n",
    "\n",
    "\n",
    " # 저장경로 생성 함수\n",
    "def mkdir(title):\n",
    "    # save_dest = '/Users/seonmin.kim/Downloads/'# for window - 'C:/Users/root/Desktop/'  # 저장을 위한 베이스 경로\n",
    "    save_dest = os.getcwd()\n",
    "    if os.path.isdir(save_dest + title):  # 베이스경로에 제목으로 하위 폴더 생성\n",
    "        pass\n",
    "    else:\n",
    "        os.makedirs(save_dest + title)\n",
    "    return save_dest + title  \n",
    "\n",
    "\n",
    "# 이미지 저장 함수\n",
    "def save_img(img_list, save_dest): \n",
    "    for i in range(0, len(img_list), 1): \n",
    "        ext_idx = (img_list[i].find('?type')) #확장자 찾기 위해 type 문자열을 먼저 찾고 \n",
    "        ext = (img_list[i][ext_idx-3:ext_idx]) #위에서 찾은 인덱스에서 -3하여 해당 파일의 확장자 확인 \n",
    "        urllib.request.urlretrieve(img_list[i], save_dest+'/'+str(i+1)+'.'+ext) \n",
    "        return len(img_list) \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### feature 함수들\n",
    "\n",
    "# 블로그 제목 추출 함수\n",
    "def get_title(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    contents = str(soup.select('.se-fs-'))  # 제목 추출을 위한 셀렉터\n",
    "    hangul = re.compile('[^ ㄱ-ㅣ가-힣]+')  # 한글만 따오기 위해\n",
    "    result = hangul.sub('', contents)\n",
    "    title = result.strip().split('  ')  # 공백으로 구분\n",
    "    return title[0]  # 0번째 인덱스가 제목 \n",
    "\n",
    "def get_blog_totalnumber(html): #블로그 전체 게시글 수\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    soup = BeautifulSoup(soup.decode(False, formatter=None), None)\n",
    "    contents=[]\n",
    "    texts = soup.find_all('h4', {'class': 'category_title pcol2'})\n",
    "    for p_span in texts:\n",
    "        # pdb.set_trace()\n",
    "        number = p_span.getText().strip().split(' ')[1][:-2]\n",
    "        contents.append(number)\n",
    "    return contents\n",
    "\n",
    "def visit_blog(url):\n",
    "    headers = { 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36',\n",
    "                'lang':'ko'}\n",
    "    html = requests.get(url, headers=headers).text\n",
    "    return html\n",
    "\n",
    "def input_url(input_url):\n",
    "    base_url = 'https://blog.naver.com/PostView.naver?blogId='  # 크롤링을 위한 베이스 주소\n",
    "    # input_url = input('블로그 주소를 입력하세요. 형식은 아래와 같습니다\\nhttps://blog.naver.com/아이디/게시글번호\\n')\n",
    "    id = input_url.split('/')[3]  # 아이디 추출\n",
    "    number = input_url.split('/')[4]  # 게시글번호 추출\n",
    "    url = base_url + id + '&logNo=' + number\n",
    "    return url\n",
    "\n",
    "\n",
    "def get_blog_number(html): #해당 블로그가 속한 메뉴의 게시물의 수\n",
    "    \n",
    "    soup1 = BeautifulSoup(html,'html.parser')\n",
    "    soup1 = BeautifulSoup(soup1.decode(False, formatter=None), None)\n",
    "    food_info = soup1.find(\"div\", {\"class\": \"blog2_series\"})\n",
    "    if food_info:\n",
    "        link = food_info.find(\"a\", {\"class\": \"pcol2\"})\n",
    "        source_url = link['href'] if link else '' \n",
    "\n",
    "    tempUrl = \"https://blog.naver.com\" + source_url\n",
    "    htm12 = visit_blog(tempUrl)\n",
    "\n",
    "    soup2 = BeautifulSoup(htm12,'html.parser')\n",
    "    soup2 = BeautifulSoup(soup2.decode(False, formatter=None), None)\n",
    "\n",
    "    contents=[]\n",
    "    texts = soup2.find_all('h4', {'class': 'category_title pcol2'})\n",
    "    for p_span in texts:\n",
    "        # pdb.set_trace()\n",
    "        temp = p_span.getText().strip().split()    \n",
    "        number = [ x[:-2] for x in temp if x[-2:] == \"개의\"]        \n",
    "        contents.append(number)\n",
    "    return contents[0]\n",
    "\n",
    "def feature_sisAutoPlayYN(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    soup = BeautifulSoup(soup.decode(False, formatter=None), None)\n",
    "    t = soup.find_all('script')\n",
    "    #print(t)\n",
    "    a = re.findall(r'\\sisAutoPlayYN\\s=\\s\\w{5}', str(t))\n",
    "    if 'true' in a:\n",
    "        a = True\n",
    "    else:\n",
    "        a = False\n",
    "    return a\n",
    "\n",
    "#블로거 프로필 info\n",
    "def get_profile_info(profile_html): #블로거 프로필 info\n",
    "    soup = BeautifulSoup(profile_html,'html.parser')\n",
    "    soup = BeautifulSoup(soup.decode(False, formatter=None), None)\n",
    "    name = []\n",
    "    texts = soup.find_all('table', {'class': 'profile_info'})\n",
    "    for text in texts:\n",
    "        name_ = text.getText().strip().split('\\n')[1]\n",
    "        name.append(name_)\n",
    "    \n",
    "    return name\n",
    "\n",
    "def get_profile_info1(profile_html): #블로거 프로필 info\n",
    "    soup = BeautifulSoup(profile_html,'html.parser')\n",
    "    soup = BeautifulSoup(soup.decode(False, formatter=None), None)\n",
    "    gender = []\n",
    "    texts = soup.find_all('table', {'class': 'profile_info'})\n",
    "    for text in texts:\n",
    "        t_gender = text.getText().strip().split('\\n')[5]\n",
    "        gender.append(t_gender)\n",
    "    \n",
    "    return gender\n",
    "\n",
    "\n",
    "\n",
    "def clean_doc(doc):\n",
    "    doc = re.sub(r'[^\\w]ㄱ-ㅎ가-힣',' ',doc)\n",
    "    return doc\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "def body_length(texts): #본문길이(공백포함)\n",
    "    temp = None\n",
    "    body_length = []\n",
    "    texts = [clean_doc(x) for x in texts]\n",
    "    texts = ''.join(texts)\n",
    "    temp = len(texts)\n",
    "    body_length.append(temp)\n",
    "    \n",
    "    return body_length\n",
    "\"\"\"\n",
    "def title_length(texts): #제목길이(단어 공백포함)\n",
    "    try:\n",
    "        temp = None\n",
    "        title_length = []\n",
    "        texts = [clean_doc(x) for x in texts]\n",
    "        texts = ''.join(texts)\n",
    "        temp = len(texts)\n",
    "        title_length.append(temp)\n",
    "    \n",
    "        return title_length\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def comment_count(html): #블로그 댓글\n",
    "    characters = \"'?!\"\n",
    "    comment_c = []\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    soup = BeautifulSoup(soup.decode(False, formatter=None), None)\n",
    "    comments = soup.find_all(\"em\", {\"id\": \"commentCount\"})\n",
    "    if comments:\n",
    "        for c in comments:\n",
    "            temp = c.getText().strip().split()\n",
    "            temp = ''.join( x for x in temp if x not in characters)\n",
    "            comment_c.append(temp)\n",
    "\n",
    "        return comment_c\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "# 블로그 제목 추출 함수\n",
    "def get_last_text(html):\n",
    "    try:\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        soup = BeautifulSoup(soup.decode(False, formatter=None), None)\n",
    "        contents = []\n",
    "        if soup.find(\"div\", attrs={\"class\":\"se-main-container\"}): # 스마트 에디터로 작성한 경우\n",
    "            texts = soup.find(\"div\", attrs={\"class\":\"se-main-container\"}).find_all('div', {'class': re.compile('^se-module se-module-tex.*')})\n",
    "            for p_span in texts:\n",
    "                for txt in p_span.find_all('span'):\n",
    "                    contents.append(txt.get_text())\n",
    "        else:\n",
    "            texts = soup.find_all('div', {'class': re.compile('^se-module se-module-tex.*')})\n",
    "            for p_span in texts:\n",
    "                for txt in p_span.find_all('span'):\n",
    "                    contents.append(txt.get_text())\n",
    "        return contents\n",
    "    except:\n",
    "        print(f\"err text\")    \n",
    "\n",
    "def profile_url(input_url):\n",
    "    base_url = 'https://blog.naver.com/profile/intro.naver?blogId='  # 크롤링을 위한 베이스 주소\n",
    "    # input_url = input('블로그 주소를 입력하세요. 형식은 아래와 같습니다\\nhttps://blog.naver.com/아이디/게시글번호\\n')\n",
    "    id = input_url.split('/')[3]  # 아이디 추출\n",
    "    profile_url = base_url + id\n",
    "    profile_html = visit_blog(profile_url)\n",
    "    return profile_html   \n",
    "\n",
    "def text_length(texts): #본문길이(공백포함)\n",
    "    try:\n",
    "        temp = None\n",
    "        body_length = []\n",
    "        texts = [clean_doc(x) for x in texts]\n",
    "        texts = ''.join(texts)\n",
    "        temp = len(texts)\n",
    "        body_length.append(temp)\n",
    "    \n",
    "        return body_length\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "\n",
    "# 블로그 해시태그 개수 및 내용 추출 함수\n",
    "def get_num_hashtags(html):\n",
    "    try:\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        soup = BeautifulSoup(soup.decode(False, formatter=None), None)\n",
    "        \n",
    "        post_footer = soup.find(\"div\", attrs={\"class\":\"post_footer_contents\"})\n",
    "        post_footer = post_footer.select(\"span\", attrs={\"class\":\"ell\"})\n",
    "        blog_hashtags = []\n",
    "        for item in post_footer:\n",
    "            parsed_item = item.get_text().strip()\n",
    "            if parsed_item != '' and parsed_item != '취소\\n확인':\n",
    "                blog_hashtags.append(parsed_item)\n",
    "        return blog_hashtags\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "# 블로그 공감 개수 추출 함수\n",
    "def get_num_likes(html):\n",
    "    try:\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        soup = BeautifulSoup(soup.decode(False, formatter=None), None)\n",
    "        \n",
    "        likeit = soup.find(\"span\", attrs={\"class\":\"u_likeit_list_btn\"})\n",
    "        likeit_count = likeit.find(\"em\", attrs={\"class\":\"u_cnt\"}).get_text()\n",
    "        return likeit_count\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "# 블로그 이웃 개수 추출 함수\n",
    "def get_num_neibors(html):\n",
    "    try:\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        soup = BeautifulSoup(soup.decode(False, formatter=None), None)\n",
    "        \n",
    "        num_neibors = soup.find(\"div\", attrs={\"id\":\"blog-stat\"})\n",
    "        num_neibors = num_neibors.find(\"ul\", attrs={\"class\":\"info\"})\n",
    "        num_neibors = num_neibors.select('em')\n",
    "        blog_stats = []\n",
    "        for item in num_neibors:\n",
    "            blog_stats.append(item.get_text().strip())\n",
    "        return blog_stats   # 블로그 이웃, 글 보내기, 글 스크랩 순으로 list에 추가\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "    \n",
    "### weekday, time_delta, datetime(post)        \n",
    "default_date = datetime(2022, 1, 1, 0, 0, 0)\n",
    "def post_datetime(html):\n",
    "    try:\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        soup = BeautifulSoup(soup.decode(False, formatter=None), None)\n",
    "        \n",
    "        if soup.find(\"span\", attrs={\"se_publishDate pcol2\"}):\n",
    "            datetime_str = soup.find(\"span\", attrs={\"se_publishDate pcol2\"}).get_text().replace(\" \" , \"\")\n",
    "        elif soup.find(\"p\", attrs={\"date fil5 pcol2 _postAddDate\"}):\n",
    "            datetime_str = soup.find(\"p\", attrs={\"date fil5 pcol2 _postAddDate\"}).get_text().replace(\" \" , \"\")\n",
    "        \n",
    "        datetime_obj = datetime.strptime(datetime_str, '%Y.%m.%d.%H:%M')\n",
    "        time_delta = (datetime_obj - default_date).days\n",
    "        post_weekday = datetime_obj.weekday()\n",
    "        return time_delta, post_weekday, str(datetime_obj) #, datetime_obj#, post_weekday\n",
    "    except:\n",
    "        return 0\n",
    "        \n",
    "### 예약하기 외부 링크 버튼 제공 여/부\n",
    "def get_reserved(html):\n",
    "    try:\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        soup = BeautifulSoup(soup.decode(False, formatter=None), None)\n",
    "        contents = []\n",
    "        texts = soup.find_all(\"a\", attrs={\"class\":\"se-placesMap-additional-button se-placesMap-button-reservation\"})\n",
    "        if texts:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except:\n",
    "        return 0\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fake_recognition(fake_result_text, fake_result_text_):\n",
    "    if fake_result_text_ or fake_result_text:\n",
    "        return 1 #print(f\"해당 블로그는 가짜 리뷰를 작성하였습니다.\")\n",
    "    else:\n",
    "        return 0 #print(f\"해당 블로그는 진짜 리뷰로 판단됩니다.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "client_id = \"wMXnGlU_z7J6RAmvtkw7\"  # <- 발급받은 client id \n",
    "client_secret = \"QOrDBMkQ6W\"        # <- 발급받은 client secret \n",
    "search_key_word = \"한남동 와인\"     #### <- 테스트 하고 싶은 검색어\n",
    "encText = urllib.parse.quote(search_key_word)   # <- search keyword \n",
    "num_display = 100\n",
    "num_start = 1\n",
    "url = \"https://openapi.naver.com/v1/search/blog?query=\" + encText \\\n",
    "        + \"&display=\" + str(num_display) + \"&start=\" + str(num_start) # json 결과\n",
    "# url = \"https://openapi.naver.com/v1/search/blog.xml?query=\" + encText # xml 결과\n",
    "request = urllib.request.Request(url)\n",
    "request.add_header(\"X-Naver-Client-Id\",client_id)\n",
    "request.add_header(\"X-Naver-Client-Secret\",client_secret)\n",
    "response = urllib.request.urlopen(request)\n",
    "rescode = response.getcode()\n",
    "\n",
    "if(rescode==200):\n",
    "    response_body = json.load(response)\n",
    "else:\n",
    "    print(\"Error Code:\" + rescode)\n",
    "\n",
    "\n",
    "url1 = [] # 0\n",
    "java_feature = [] # 1\n",
    "contents = [] # 2\n",
    "profile_html = [] # 3\n",
    "name = [] # 4\n",
    "gender = [] # 5\n",
    "size_text = [] # 6\n",
    "size_title = [] # 7\n",
    "com_count = [] # 8\n",
    "num_neibors = [] #9\n",
    "num_likes = [] #10\n",
    "num_hashtags = [] #11\n",
    "img_list_size = []\n",
    "img_sticker_size = []\n",
    "fake_result = []\n",
    "post_timedelta = []\n",
    "post_week = []\n",
    "post_date = []\n",
    "get_reserved_link = []\n",
    "output = pd.DataFrame()\n",
    "\n",
    "### [:10] 이 부분에서 불러오는 개수 설정 가능\n",
    "for item in response_body['items'][:5]:\n",
    "\n",
    "    # url = input_url(item['link'])\n",
    "    # html = visit_blog(url)\n",
    "    # title = get_title(html)\n",
    "    # texts = get_last_text(html)\n",
    "    # java_feature.append(feature_sisAutoPlayYN(html))\n",
    "    # contents.append(get_blog_totalnumber(html))\n",
    "    # profile_html = profile_url(item['link'])\n",
    "    # name.append(get_profile_info(profile_html))\n",
    "    # gender.append(get_profile_info1(profile_html))\n",
    "    # size_text.append(text_length(texts))\n",
    "    # size_title.append(title_length(title))\n",
    "    # com_count.append(comment_count(html))\n",
    "\n",
    "\n",
    "    url = input_url(item['link'])\n",
    "    html = visit_blog(url)\n",
    "    title = get_title(html)\n",
    "    texts = get_last_text(html)\n",
    "    java_feature = (feature_sisAutoPlayYN(html))\n",
    "    contents = (get_blog_totalnumber(html))\n",
    "    profile_html = profile_url(item['link'])\n",
    "    name = (get_profile_info(profile_html))\n",
    "    gender = (get_profile_info1(profile_html))\n",
    "    size_text = (text_length(texts))\n",
    "    size_title = (title_length(title))\n",
    "    com_count = (comment_count(html))\n",
    "    num_neibors = (get_num_neibors(html))\n",
    "    num_likes = (get_num_likes(html))\n",
    "    num_hashtags = len(get_num_hashtags(html))\n",
    "    img_list, img_sticker_list = get_img_list(html)\n",
    "    img_list_size = len(img_list)\n",
    "    img_sticker_size = len(img_sticker_list)\n",
    "    last_ten_texts = print_last_text(texts, 10)\n",
    "    post_timedelta, post_week, post_date = post_datetime(html)\n",
    "    get_reserved_link = get_reserved(html)\n",
    "    fake_result_text_ = contain_fake_keyword(last_ten_texts)\n",
    "    fake_result_text = contain_fake_image_keyword(img_list, img_sticker_list)\n",
    "    fake_result = fake_recognition(fake_result_text, fake_result_text_)\n",
    "\n",
    "    \n",
    "\n",
    "    new_df = pd.DataFrame({\n",
    "    \"url\" : url,\n",
    "    \"java_feature\" : java_feature,\n",
    "    \"contents\" : contents,\n",
    "   # \"profile_html\" : profile_html,\n",
    "    \"name\" : name,\n",
    "    \"gender\" : gender,\n",
    "    \"size_text\" : size_text,\n",
    "    \"size_title\" : size_title,\n",
    "    \"com_count\" : com_count,\n",
    "    \"num_neibors\" : num_neibors,\n",
    "    \"num_likes\" : num_likes,\n",
    "    \"num_hashtags\" : num_hashtags,\n",
    "    \"img_list_size\" : img_list_size,\n",
    "    \"img_sticker_size\" : img_sticker_size,\n",
    "    \"post_timedelta\" : post_timedelta,\n",
    "    \"post_week\" : post_week,\n",
    "    \"post_date\" : post_date,\n",
    "    \"get_reserved_link\": get_reserved_link,\n",
    "    \"fake_result\" : fake_result\n",
    "        \n",
    "    })\n",
    "\n",
    "    output = output.append(new_df, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>java_feature</th>\n",
       "      <th>contents</th>\n",
       "      <th>name</th>\n",
       "      <th>gender</th>\n",
       "      <th>size_text</th>\n",
       "      <th>size_title</th>\n",
       "      <th>com_count</th>\n",
       "      <th>num_neibors</th>\n",
       "      <th>num_likes</th>\n",
       "      <th>num_hashtags</th>\n",
       "      <th>img_list_size</th>\n",
       "      <th>img_sticker_size</th>\n",
       "      <th>post_timedelta</th>\n",
       "      <th>post_week</th>\n",
       "      <th>post_date</th>\n",
       "      <th>get_reserved_link</th>\n",
       "      <th>fake_result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://blog.naver.com/PostView.naver?blogId=e...</td>\n",
       "      <td>False</td>\n",
       "      <td>352</td>\n",
       "      <td>비공개</td>\n",
       "      <td>비공개</td>\n",
       "      <td>2493</td>\n",
       "      <td>15</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>256</td>\n",
       "      <td>2</td>\n",
       "      <td>2022-09-14 00:23:00</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://blog.naver.com/PostView.naver?blogId=k...</td>\n",
       "      <td>False</td>\n",
       "      <td>3,464</td>\n",
       "      <td>비공개</td>\n",
       "      <td>비공개</td>\n",
       "      <td>2479</td>\n",
       "      <td>25</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>250</td>\n",
       "      <td>3</td>\n",
       "      <td>2022-09-08 15:00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://blog.naver.com/PostView.naver?blogId=b...</td>\n",
       "      <td>False</td>\n",
       "      <td>1,948</td>\n",
       "      <td>비공개</td>\n",
       "      <td>비공개</td>\n",
       "      <td>2304</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>304</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-11-01 00:53:00</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://blog.naver.com/PostView.naver?blogId=l...</td>\n",
       "      <td>False</td>\n",
       "      <td>2,081</td>\n",
       "      <td>비공개</td>\n",
       "      <td>비공개</td>\n",
       "      <td>1773</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>295</td>\n",
       "      <td>6</td>\n",
       "      <td>2022-10-23 02:42:00</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://blog.naver.com/PostView.naver?blogId=h...</td>\n",
       "      <td>False</td>\n",
       "      <td>625</td>\n",
       "      <td>비공개</td>\n",
       "      <td>비공개</td>\n",
       "      <td>2126</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>6</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>290</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-10-18 06:54:00</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  java_feature contents  \\\n",
       "0  https://blog.naver.com/PostView.naver?blogId=e...         False      352   \n",
       "1  https://blog.naver.com/PostView.naver?blogId=k...         False    3,464   \n",
       "2  https://blog.naver.com/PostView.naver?blogId=b...         False    1,948   \n",
       "3  https://blog.naver.com/PostView.naver?blogId=l...         False    2,081   \n",
       "4  https://blog.naver.com/PostView.naver?blogId=h...         False      625   \n",
       "\n",
       "   name gender  size_text  size_title com_count  num_neibors num_likes  \\\n",
       "0   비공개    비공개       2493          15                      0             \n",
       "1   비공개    비공개       2479          25        11            0             \n",
       "2   비공개    비공개       2304          13         1            0             \n",
       "3   비공개    비공개       1773          22         3            0             \n",
       "4   비공개    비공개       2126          28        28            0             \n",
       "\n",
       "   num_hashtags  img_list_size  img_sticker_size  post_timedelta  post_week  \\\n",
       "0             0             20                 0             256          2   \n",
       "1             0             34                 0             250          3   \n",
       "2             0             32                 1             304          1   \n",
       "3             6             22                 0             295          6   \n",
       "4             6             32                 2             290          1   \n",
       "\n",
       "             post_date  get_reserved_link  fake_result  \n",
       "0  2022-09-14 00:23:00              False            0  \n",
       "1  2022-09-08 15:00:00              False            0  \n",
       "2  2022-11-01 00:53:00              False            1  \n",
       "3  2022-10-23 02:42:00              False            0  \n",
       "4  2022-10-18 06:54:00              False            1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "c261aea317cc0286b3b3261fbba9abdec21eaa57589985bb7a274bf54d6cc0a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
